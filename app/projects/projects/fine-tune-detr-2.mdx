---
title: 'Fine-tune a Vision Transformer on Synthetic Data, Part 2: Fine-tune and Evaluate your model'
publishedAt: '2025-12-14'
summary: 'Explore the enduring debate between using spaces and tabs for code indentation, and why this choice matters more than you might think.'
---

_This is an updated version of the blog I made years ago on generating synthetic data with Blender to fine-tune a YOLO model. If you're looking for that blog, you can find it [here](https://federicoarenasl.github.io/sdg-engine/)._

_This section of the tutorial is currently under development. Thank you for your patience as I work to make it even better!_

### Introduction

This tutorial will walk you through fine-tuning a Vision Transformer on synthetic data to perform object detection.

We'll use the [sdg-engine](https://github.com/federicoarenasl/sdg-engine) package to generate the data with Blender, fine-tune a DEtection TRansformer (DETR) model using the HuggingFace ðŸ¤— `transformers` and `datasets` libraries, and validate its performance against one of the open source _"BOP: Benchmark for 6D Object Pose Estimation"_ benchmark datasets [1].

This tutorial is divided in two parts:
1. [**Part 1: Generate data using Blender**](/projects/train-detr-on-synthetic-data-1): this part focuses on setting up your Blender scene, your `sdg-package`, generating your fine-tuning data, and pushing your `train` and `validation` datasets into the HuggingFace Hub.
2. [**Part 2: Fine-tune DETR on Synthetic Data**](/projects/train-detr-on-synthetic-data-2): this part focuses on taking your `train`, and `validation` datasets and fine-tuning DETR using the HuggingFace `transformers` library.

By the end of this tutorial, you'll have created a synthetic dataset using Blender, fine-tuned a Vision Transformer on it, and evaluated its performance on an open source dataset â€” all with HuggingFace ðŸ¤—.

If you have any questions or comments regarding these tutorials, please let me know at [github.com/federicoarenasl/sdg-engine/issues](https://github.com/federicoarenasl/sdg-engine/issues).

Let's dive right in!

## Prerequisites
Before we start, you'll need an NVIDIA or Apple Silicon GPU in order to fine-tune DETR.

The full tutorial can be run from the following notebook (INSERT LINK), which you can run from a GPU-enabled Google Colab notebook, or from your own GPU-enabled machine.

## i. The BOP Homebrew Dataset
To keep things reproducible, we'll use an open source dataset as our ground truth for evaluation. I've picked the Homebrew dataset from the widely known Benchmark for 6D Object Pose Estimation (BOP). 

The Homebrew dataset has a multitude of scenes with different objects rotating on a platform, and for simplicity we'll fine-tune our model to detect objects in only one of their scenes -- scene No. 3 -- shown below:

<Image
  src="/hb-dataset-sample.gif"
  alt="Sample from the Homebrew BOP dataset showing 4 different objects on a rotating platform"
  width={480}
  height={315}
/>


This scene contains 4 different objects, set up on a rotating platform under constant lighting. The resolution of the images is 640px by 420px, and the scene itself contains close to 300 images.

If you want to learn more about the BOP benchmark or the Homebrew dataset, click here or here (INSERT LINKS)!

## iv. Fine-tune DETR using the `transformers` library
Now that we have both our evaluation and our training datasets somewhere safely in the HuggingFace Hub, we are ready to use them to fine-tune our DEtection TRansformer!

### Loading the datasets
Both of our datasets now hopefully live in the HF Hub, so pulling them is super simple, we can load our synthetic dataset as `train_dataset`, from which we'll only use the `'train'` split:

```python
from datasets import load_dataset

synthetic_dataset = load_dataset(
    "federicoarenas-ai/synthetic-bop-homebrew-scene-3-medium", 
    token=os.environ['HF_TOKEN']
    )
train_dataset = synthetic_dataset['train']
```

We can do the same for our real-world validation dataset, by loading it into `val_dataset` and only taking the `'validation'` split:

```python
real_dataset = load_dataset(
    "federicoarenas-ai/real-bop-homebrew-scene-3", 
    token=os.environ['HF_TOKEN']
    )
val_dataset = real_dataset['validation']
```

### Prepare the image embedding model
Next, we'll need to create an `image_processor` which will take our raw images and embed them into vectors that are consumable by our `transformers` DETR model. 

```python
from transformers import AutoImageProcessor

checkpoint = "facebook/detr-resnet-50-dc5"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

> **Tip:** By starting from a pretrained checkpoint, our `image_processor` leverages previously learned visual features, allowing the model to focus on adapting to our specific task rather than learning basic image representations from scratch. This typically leads to faster convergence and improved performance.

### Augment the data
Data augmentations are a common thing in any ML fine-tuning pipeline, they let us incentivize generalization in our model via data transformations in the training dataset. We'll prepare these using the albumentations library. This library is awesome, you can `albumentations.Compose` a series of augmentations which will then be applied to each of the datasets.

Let's introduce transformations in our training dataset to make the model more robust to rotational variations, brightness and contrast variations, noise variations, and other variations in the images:

```python
import albumentations as A

train_transform = A.Compose(
    [
        A.LongestMaxSize(500),
        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.HueSaturationValue(p=0.5),
        A.Rotate(limit=10, p=0.5),
        A.RandomScale(scale_limit=0.2, p=0.5),
        A.GaussianBlur(p=0.5),
        A.GaussNoise(p=0.5),
    ],
    bbox_params=A.BboxParams(format="coco", label_fields=["categories"]),
)
```
In the case of our validation dataset, we'll want to keep it as close to the real-world distribution as possible, so we'll only apply transformations that homogenize the input data, e.g. ensuring the max size of the image stays within 500px and making sure that we pad all images up to the 500px by 500px mark:
```python
test_transform = A.Compose(
    [
        A.LongestMaxSize(500),
        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),
    ],
    bbox_params=A.BboxParams(format="coco", label_fields=["categories"]),
)
```

> **Note:** When we apply transformations to the images, we need to apply transformations to the annotations as well, otherwise we'd have outdated labels for our transformed images. Hence we set the `bbox_params` to transform our COCO bounding box annotations stored in our `"categories"` field.

Now to apply the transformations we'll need a function that takes in any given sample in our HF `dataset`, goes through all bounding boxes in the sample, applies the transformation, and finally returns the embedded image ready for ingestion:

```python

def transform_aug_ann(sample: dict, transform: A.BasicTransform):
    """Applies trainsformation to the provided sample."""
    image_ids = sample["image_id"]
    images, bboxes, categories, area = [], [], [], []
    for image, objects in zip(sample["image"], sample["objects"]):
        # Convert image to RGB and reverse the color channel to BGR
        image = np.array(image.convert("RGB"))[:, :, ::-1]

        # Apply the transformation
        out = transform(image=image, bboxes=objects["bbox"], categories=objects["categories"])
        
        # Collect data
        area.append(objects["areas"])
        images.append(out["image"])
        bboxes.append(out["bboxes"])
        categories.append(out["categories"])

    # Format the annotations as COCO-formatted annotations
    targets = [
        {"image_id": id_, "annotations": formatted_anns(id_, cat_, box_, area_)}
        for id_, cat_, box_, area_ in zip(image_ids, categories, bboxes, area)
    ]

    return image_processor(images=images, annotations=targets, return_tensors="pt")
```

Finally this function can be mapped to all samples in both dataset via the `.with_transform()` method, handy from the HF `datasets`:

```python
def transform_train(sample: dict):
    """Apply the `train_transform` to any given sample."""
    return transform_aug_ann(sample, transform=train_transform)


def transform_test(sample):
    """Apply the `test_transform` to any given sample."""
    return transform_aug_ann(sample, transform=test_transform)

    
# Apply the transformations
train_dataset_transformed = train_dataset.with_transform(transform_train)
test_dataset_transformed = test_dataset.with_transform(transform_test)
```

Awesome! We now have our `train_dataset_transformed` and our `val_dataset_transform` ready to use during fine-tuning and validation.

### Prepare evaluation metrics

To evaluate our model's performance, we'll use the Mean Average Precision (mAP) metric from TorchMetrics. The evaluation setup requires three functions that work together to process predictions and compute metrics.

The main `compute_metrics` function accumulates predictions across batches and computes final metrics at the end of evaluation:

```python
_batch_cache: list[dict] = []

def compute_metrics(eval_pred, compute_result: bool):
    """Stateless per-batch call OR final aggregation when compute_result=True."""
    global _batch_cache

    preds, targets = process_batch(eval_pred)

    if not compute_result:
        # accumulate batch outputs
        _batch_cache.append({"preds": preds, "targets": targets})
        return {}
    else:
        # aggregate everything
        all_preds, all_targets = [], []
        for batch in _batch_cache:
            all_preds.extend(batch["preds"])
            all_targets.extend(batch["targets"])

        metrics = aggregate_and_compute(all_preds, all_targets, ID_TO_LABEL)

        # clear cache for next eval
        _batch_cache = []
        return metrics
```

The `process_batch` function converts raw model outputs into a format compatible with TorchMetrics:

```python
def process_batch(eval_pred) -> tuple[list[dict], list[dict]]:
    """Turn one batch from Trainer into TorchMetrics-friendly preds/targets."""
    (_, scores, pred_boxes, _, _), labels = eval_pred
    preds, targets = [], []

    for i, (label, pbox, sc) in enumerate(zip(labels, pred_boxes, scores)):
        try:
            w, h = label["orig_size"]
            targets.append(build_target(label))
            preds.append(build_prediction(pbox, sc, w, h))
        except Exception as e:
            warnings.warn(f"[process_batch] Skipping sample {i}: {e}")
            continue

    return preds, targets

```

Finally, `aggregate_and_compute` calculates overall and per-class metrics:

```python
def aggregate_and_compute(
    all_preds: list[dict], 
    all_targets: list[dict], 
    id_to_label: dict | None = None
) -> dict:
    """Compute mAP and flatten per-class metrics."""
    metric = MeanAveragePrecision(box_format="xywh", class_metrics=True)
    metric.update(preds=all_preds, target=all_targets)
    metrics = metric.compute()

    # Expand class-wise metrics
    classes = metrics.pop("classes")
    map_per_class = metrics.pop("map_per_class")
    mar_100_per_class = metrics.pop("mar_100_per_class")

    for cls_id, cls_map, cls_mar in zip(classes, map_per_class, mar_100_per_class):
        cls_name = id_to_label[cls_id.item()] if id_to_label else str(cls_id.item())
        metrics[f"map_{cls_name}"] = cls_map
        metrics[f"mar_100_{cls_name}"] = cls_mar

    return {k: round(v.item(), 4) for k, v in metrics.items()}
```

### Prepare the collate function

The collate function combines individual samples into batches for fine-tuning. It ensures all images are properly padded and formatted:

```python
def collate_fn(batch: list[torch.Tensor])-> torch.Tensor:
    """Collate function preparing a batch of samples."""
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]

    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels

    return batch
```

### Initialize logging using Weights and Biases

To track our fine-tuning metrics and visualize results, we'll use Weights & Biases. Set up logging with a unique run identifier:

```python
import wandb

UNIQUE_RUN_IDENTIFIER = f"vit-training-run-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"

# Login to Weights and Biases
wandb.login()

# Initialize W&B run
wandb.init(
    id=UNIQUE_RUN_IDENTIFIER,
    project="train-detr",
    name=UNIQUE_RUN_IDENTIFIER,
    resume=True,
    config=training_args,
)
```

### Initialize the model and fine-tuning configuration

With our datasets prepared and evaluation metrics configured, we can now initialize the DETR model and set up the fine-tuning parameters.

First, load the pretrained DETR model and configure it for our specific object classes:

```python
from transformers import DetrForObjectDetection, TrainingArguments
import datetime

# Prepare model
model = DetrForObjectDetection.from_pretrained(
    checkpoint,
    id2label=ID_TO_LABEL,
    label2id=LABEL_TO_ID,
    ignore_mismatched_sizes=True,
)
```

Next, define the fine-tuning hyperparameters. These values control the learning process, batch sizes, evaluation frequency, and logging:

```python
# Define the training arguments
training_args = TrainingArguments(
    output_dir=UNIQUE_RUN_IDENTIFIER,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    max_steps=10000,
    save_steps=10,
    logging_steps=1,
    learning_rate=1e-5,
    weight_decay=1e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    eval_steps=100,
    eval_strategy="steps",
    report_to="wandb",
    batch_eval_metrics=True,
)
```

Now create the `Trainer` object that orchestrates the fine-tuning loop, combining the model, datasets, and configuration:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_dataset_transformed,
    eval_dataset=test_dataset_transformed,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
)
```

Finally, start the fine-tuning process:

```python
trainer.train()
```

The model will be fine-tuned for 10,000 steps, evaluating on the real-world validation set every 100 steps and logging metrics to Weights & Biases.

#### iv-4. Fine-tuning run
After running the fine-tuning for 10,000 steps, we can observe the model's learning progression through several key metrics.

The fine-tuning loss shows a steady decline throughout the process, indicating that the model is effectively adapting from the synthetic data:

<Image
  src="/train_loss.svg"
  alt="Fine-tuning loss curve showing steady decline over 10,000 steps"
  width={500}
  height={315}
/>

The evaluation loss on the real-world Homebrew dataset also decreases, suggesting that the model generalizes well from synthetic to real-world data:

<Image
  src="/eval_loss.svg"
  alt="Evaluation loss curve on real-world validation data"
  width={500}
  height={315}
/>

The mean Average Precision (mAP) metric steadily improves during fine-tuning, demonstrating that the model's detection accuracy is increasing:

<Image
  src="/eval_map.svg"
  alt="Mean Average Precision (mAP) metric progression during fine-tuning"
  width={500}
  height={315}
/>

Here's a demonstration of the fine-tuned model detecting objects in the real-world Homebrew dataset:

<Image
  src="/trained_demo.gif"
  alt="Fine-tuned DETR model detecting objects in the Homebrew dataset"
  width={500}
  height={315}
/>


## v. What did we learn today?

In this tutorial, we successfully fine-tuned a DETR model using entirely synthetic data generated from Blender and evaluated it on real-world images from the BOP Homebrew dataset. The key takeaways include:

- **Synthetic-to-real transfer is feasible**: Models fine-tuned on synthetic data can generalize to real-world scenarios when the synthetic data captures sufficient visual diversity.
- **Data augmentation matters**: Applying transformations like rotation, brightness adjustments, and noise helps bridge the gap between synthetic and real domains.
- **Proper evaluation is critical**: Using established metrics like mAP and evaluating on held-out real-world data ensures the model is learning meaningful features rather than memorizing synthetic artifacts.
- **Blender as a data generator**: With the right setup, Blender can serve as a powerful tool for generating labeled training data, especially for scenarios where real-world data collection is expensive or impractical.

## vi. Where to go from here

There are several directions you can explore to build upon this tutorial:

- **Experiment with domain randomization**: Add more variability to lighting, backgrounds, and object textures in your Blender scenes to improve generalization.
- **Try different architectures**: Test other object detection models like YOLOv8, Faster R-CNN, or newer vision transformers to compare performance.
- **Increase dataset size**: Generate more synthetic samples to see how model performance scales with data quantity.
- **Fine-tune on real data**: Use the synthetic-fine-tuned model as a starting point and fine-tune it with a small amount of real-world labeled data.
- **Explore other BOP scenes**: Apply the same pipeline to other scenes or datasets in the BOP benchmark to test generalization across different object sets.
- **Implement domain adaptation**: Look into techniques like adversarial training or style transfer to further reduce the synthetic-to-real domain gap.