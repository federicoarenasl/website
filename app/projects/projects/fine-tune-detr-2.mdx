---
title: 'Fine-tune DETR on Synthetic Data, Part 2'
publishedAt: '2025-12-14'
summary: 'Explore the enduring debate between using spaces and tabs for code indentation, and why this choice matters more than you might think.'
---

Synthetic data is a powerful tool to get Machine Learning practitioners off the ground fast into a working model that can perform the task sufficiently well. In this two series we'll do just that for training Computer Vision model. 

We'll pick a concrete real-world object detection task and train a Vision Transformer using synthetically generated image data using Blender, while making sure we are evaluating this model on the real-world target distribution.

### Introduction

This is Part 2 of my tutorial series on fine-tuning a Vision Transformer on synthetic data to perform object detection. By this point you should have a synthetically generated train dataset and a real-world validation dataset sitting on your own HuggingFace Hub.

If you don't have these, you can go back to [**Part 1: Generate data using Blender**](/projects/fine-tune-detr-1) where you'll be able to prepare your training and validation data.

If you have any questions or comments regarding these tutorials, please let me know at [github.com/federicoarenasl/sdg-engine/issues](https://github.com/federicoarenasl/sdg-engine/issues).

Let's dive right in!

## Fine-tune DETR using the `transformers` library
Now that we have both our evaluation and our training datasets somewhere safely in the HuggingFace Hub, we are ready to use them to fine-tune our DEtection TRansformer!

### Load the datasets
Both of our datasets now hopefully live in the HF Hub, so pulling them is super simple, we can load our synthetic dataset as `train_dataset`, from which we'll only use the `'train'` split:

```python
from datasets import load_dataset

# Load synthetic `train_dataset`
synthetic_dataset = load_dataset(
    "federicoarenas-ai/synthetic-bop-homebrew-scene-3-large", 
    token="" # Yout HF token
    )
train_dataset = synthetic_dataset['train']

# Load real-world `val_dataset`
real_dataset = load_dataset(
    "federicoarenas-ai/real-bop-homebrew-scene-3", 
    token="" # Your HF token
    )
val_dataset = real_dataset['validation']
```

### Prepare the image embedding model
Next, we'll need to create an `image_processor` which will take our raw images and embed them into vectors that are consumable by our `transformers` DETR model. 

```python
from transformers import AutoImageProcessor

checkpoint = "facebook/detr-resnet-50-dc5"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

By starting from a pretrained checkpoint, our `image_processor` leverages previously learned visual features, allowing the model to focus on adapting to our specific task rather than learning basic image representations from scratch. This typically leads to faster convergence and improved performance.

### Augment the data
Data augmentations are a common thing in any ML fine-tuning pipeline, they let us incentivize generalization in our model via data transformations in the training dataset.

Let's introduce transformations in our training dataset to make the model more robust to rotational variations, brightness and contrast variations, noise variations, and other variations in the images:

```python
import albumentations as A

train_transform = A.Compose(
    [
        A.LongestMaxSize(500),
        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.HueSaturationValue(p=0.5),
        A.Rotate(limit=10, p=0.5),
        A.RandomScale(scale_limit=0.2, p=0.5),
        A.GaussianBlur(p=0.5),
        A.GaussNoise(p=0.5),
    ],
    bbox_params=A.BboxParams(format="coco", label_fields=["categories"]),
)
```
In the case of our validation dataset, we'll want to keep it as close to the real-world distribution as possible, so we'll only apply transformations that homogenize the input data, like ensuring the max size of the image stays within 500px by 500px size:
```python
test_transform = A.Compose(
    [
        A.LongestMaxSize(500),
        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),
    ],
    bbox_params=A.BboxParams(format="coco", label_fields=["categories"]),
)
```

> **Note:** When we apply transformations to the images, we need to apply transformations to the annotations as well, otherwise we'd have outdated labels for our transformed images. Hence we set the `bbox_params` to transform our COCO bounding box annotations stored in our `"categories"` field.

Now to apply the transformations we'll need a function that takes in any given sample in our HF `dataset`, goes through all bounding boxes in the sample, applies the transformation, and finally returns the embedded image ready for ingestion:

```python
def transform_aug_ann(sample: dict, transform: A.BasicTransform):
    """Applies trainsformation to the provided sample."""
    image_ids = sample["image_id"]
    images, bboxes, categories, area = [], [], [], []
    for image, objects in zip(sample["image"], sample["objects"]):
        # Convert image to RGB and reverse the color channel to BGR
        image = np.array(image.convert("RGB"))[:, :, ::-1]

        # Apply the transformation
        out = transform(image=image, bboxes=objects["bbox"], categories=objects["categories"])
        
        # Collect data
        area.append(objects["areas"])
        images.append(out["image"])
        bboxes.append(out["bboxes"])
        categories.append(out["categories"])

    # Format the annotations as COCO-formatted annotations
    targets = [
        {"image_id": id_, "annotations": formatted_anns(id_, cat_, box_, area_)}
        for id_, cat_, box_, area_ in zip(image_ids, categories, bboxes, area)
    ]

    return image_processor(images=images, annotations=targets, return_tensors="pt")
```

Finally this function can be mapped to all samples in both dataset via the `.with_transform()` method, handy from the HF `datasets` library:

```python
def transform_train(sample: dict):
    """Apply the `train_transform` to any given sample."""
    return transform_aug_ann(sample, transform=train_transform)


def transform_test(sample):
    """Apply the `test_transform` to any given sample."""
    return transform_aug_ann(sample, transform=test_transform)

# Apply the transformations
train_dataset_transformed = train_dataset.with_transform(transform_train)
test_dataset_transformed = test_dataset.with_transform(transform_test)
```

Awesome! We now have our `train_dataset_transformed` and our `val_dataset_transform` ready to use during fine-tuning and validation.

### Prepare the evaluation metrics

To evaluate our model's performance, we'll use the Mean Average Precision (mAP) metric from [TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/). The evaluation setup requires three functions that work together to process predictions and compute metrics.

The `process_batch` function converts raw model outputs into a format compatible with TorchMetrics:

```python
def process_batch(eval_pred) -> tuple[list[dict], list[dict]]:
    """Turn one batch from Trainer into TorchMetrics-friendly preds/targets."""
    (_, scores, pred_boxes, _, _), labels = eval_pred
    preds, targets = [], []

    for i, (label, pbox, sc) in enumerate(zip(labels, pred_boxes, scores)):
        try:
            w, h = label["orig_size"]
            targets.append(build_target(label))
            preds.append(build_prediction(pbox, sc, w, h))
        except Exception as e:
            warnings.warn(f"[process_batch] Skipping sample {i}: {e}")
            continue

    return preds, targets

```

Next, `aggregate_and_compute` calculates overall and per-class metrics:

```python
def aggregate_and_compute(
    all_preds: list[dict], 
    all_targets: list[dict], 
    id_to_label: dict | None = None
) -> dict:
    """Compute mAP and flatten per-class metrics."""
    metric = MeanAveragePrecision(box_format="xywh", class_metrics=True)
    metric.update(preds=all_preds, target=all_targets)
    metrics = metric.compute()

    # Expand class-wise metrics
    classes = metrics.pop("classes")
    map_per_class = metrics.pop("map_per_class")
    mar_100_per_class = metrics.pop("mar_100_per_class")

    for cls_id, cls_map, cls_mar in zip(classes, map_per_class, mar_100_per_class):
        cls_name = id_to_label[cls_id.item()] if id_to_label else str(cls_id.item())
        metrics[f"map_{cls_name}"] = cls_map
        metrics[f"mar_100_{cls_name}"] = cls_mar

    return {k: round(v.item(), 4) for k, v in metrics.items()}
```

Finally, we use these two functions in `compute_metrics`, where we accumulate predictions across batches and compute the final, aggregated metrics at the end of evaluation:

```python
_batch_cache: list[dict] = []

def compute_metrics(eval_pred, compute_result: bool):
    """Stateless per-batch call OR final aggregation when compute_result=True."""
    global _batch_cache

    preds, targets = process_batch(eval_pred)

    if not compute_result:
        # accumulate batch outputs
        _batch_cache.append({"preds": preds, "targets": targets})
        return {}
    else:
        # aggregate everything
        all_preds, all_targets = [], []
        for batch in _batch_cache:
            all_preds.extend(batch["preds"])
            all_targets.extend(batch["targets"])

        metrics = aggregate_and_compute(all_preds, all_targets, ID_TO_LABEL)

        # clear cache for next eval
        _batch_cache = []
        return metrics
```

## Prepare the fine-tuning run
#### Initialize logging using Weights and Biases

To track our fine-tuning metrics and visualize results, we'll use Weights & Biases. Set up logging with a unique run identifier:

```python
import wandb

UNIQUE_RUN_IDENTIFIER = f"vit-training-run-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"

# Login to Weights and Biases
wandb.login()

# Initialize W&B run
wandb.init(
    id=UNIQUE_RUN_IDENTIFIER,
    project="train-detr",
    name=UNIQUE_RUN_IDENTIFIER,
    resume=True,
    config=training_args,
)
```
#### Prepare the collate function

The collate function combines individual samples into batches for fine-tuning. It ensures all images are properly padded and formatted:

```python
def collate_fn(batch: list[torch.Tensor])-> torch.Tensor:
    """Collate function preparing a batch of samples."""
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]

    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels

    return batch
```

#### Initialize the model and fine-tuning configuration

With our datasets prepared and evaluation metrics configured, we can now initialize the DETR model and set up the fine-tuning training parameters.

First, load the pretrained DETR model and configure it for our specific object classes:

```python
from transformers import DetrForObjectDetection, TrainingArguments
import datetime

#### Prepare model
model = DetrForObjectDetection.from_pretrained(
    checkpoint,
    id2label=ID_TO_LABEL,
    label2id=LABEL_TO_ID,
    ignore_mismatched_sizes=True,
)
```

Next, define the fine-tuning hyperparameters. These values control the learning process, batch sizes, evaluation frequency, and logging:

```python
# Define the training arguments
training_args = TrainingArguments(
    output_dir=UNIQUE_RUN_IDENTIFIER,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    max_steps=10000,
    save_steps=10,
    logging_steps=1,
    learning_rate=1e-5,
    weight_decay=1e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    eval_steps=100,
    eval_strategy="steps",
    report_to="wandb",
    batch_eval_metrics=True,
)
```

Our fine-tuning hyperparameter consist of a small learning rate intended to slightly update the weights of our pretrained checkpoint, and L2 regularization to penalize large weights during training.  

Furthermore, with these `TrainingArguments` we set up a training run running for 10000 iterations, saving a checkpoint of the trained weights every 10 iterations, running evaluation every 100 iterations, and logging to Weights & Biases every 1 iteration. 

> Note: These are initial training settings, and hyperparameters set by me. I encourage you to try your own to improve fine-tuning performance!

## Fine-tune your model
Now create the `Trainer` object that orchestrates the fine-tuning loop, combining the model, datasets, and configuration:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_dataset_transformed,
    eval_dataset=test_dataset_transformed,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
)
```

Finally, start the fine-tuning process:

```python
trainer.train()
```

The model will be fine-tuned for 10000 iterations, evaluating on the real-world validation set every 100 steps and logging metrics to Weights & Biases.

<Image
  src="/train_loss.png"
  alt="Fine-tuning loss curve showing steady decline over 10,000 steps"
  width={500}
  height={315}
/>

Much like the plot above, you should see the training loss showing a steady decline throughout the process, indicating that the model is effectively adapting from the synthetic data. The training loss is looking jumpy, and we might benefit from reducing the learning rate furthermore, or reducing the batch size used to train our model at every iteration.

#### Evaluation results
Encouragingly, by looking at the evaluation loss calculated against the real-world validation dataset, our evaluation loss shows a steady decline, indicating that our DETR model is able to generalize to a real-world domain, when trained using synthetic data generated with our `sdg-engine` package.

<Image
  src="/eval_loss.png"
  alt="Evaluation loss on real-world validation data"
  width={500}
  height={315}
/>

Moreover, by looking at our mean Average Precision (mAP) we are able to see how well the model is doing at correctly predicting the correct classes across our entire validation dataset. 
<Image
  src="/eval_map.png"
  alt="Mean Average Precision (mAP) metric progression during fine-tuning"
  width={500}
  height={315}
/>

Even though we see a steady improvement, the model's performance saturates at close to `mAP=0.67`, this is enough to demonstrate that our model is able to generalize to a real-world dataset, but well below a production-ready model that can be used reliably.


<Image
  src="/trained_demo.gif"
  alt="Fine-tuned DETR model detecting objects in the Homebrew dataset"
  width={500}
  height={315}
/>

From the qualitative results above, we see that our model is able to detect the Minion and the Butter quite well, but struggles with the Toy Cow and the Stapler! From further investigation, this is clearly reflected in our model's mean Average Precision for each individual object.

<Image
  src="/individual_eval_map.png"
  alt="Our trained model's mAP across each of the classes"
  width={500}
  height={315}
/>

Clearly our model's struggling with the Toy Cow and the Stapler, a key area for improvement.  When improving our model's performance, we can focus on improving performance on these two classes with a few strategies in mind; either (1) upsampling data from these underperforming classes, (2) generating more diverse data containing both of these objects, or by (3) improving the rendering quality of both of these classes directly in Blender.

## Conclusions
Synthetic data is a powerful tool to get Machine Learning practitioners off the ground fast into a working model that can perform the task sufficiently well. In this two series tutorial we did just that. We went from a concrete real-world task we wanted to train a Vision Transformer on, to generating the data to train this model, while making sure we were evaluating this model on the real-world target distribution.

We used my own package `sdg-engine` to facilitate the synthetic data generation process using Blender, and we use the HuggingFace `datasets` and `transformers` libraries to facilitate the dataset management process, and the fine-tuning process of a DETR model, a specialized transformer designed specifically for object detection.

Finally, we monitored training and evaluation performance using Weights & Biases, and concluded with a class-based error analysis identifying key areas of improvement. I encourage you to outperform my base model and share the strategies you followed to improve mean Average Precision!

